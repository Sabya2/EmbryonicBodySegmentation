{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7669043,"sourceType":"datasetVersion","datasetId":3448145},{"sourceId":7690030,"sourceType":"datasetVersion","datasetId":4157240},{"sourceId":11724,"sourceType":"modelInstanceVersion","modelInstanceId":9467},{"sourceId":11985,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":9707}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sabyasachi96/myofarm-pyimage?scriptVersionId=166375005\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# All the imports\n!pip install patchify\n!pip install GPUtil\n!pip install torchsummary\n\nfrom GPUtil import showUtilization as gpu_usage\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n# import seaborn as sns\nimport os\nimport cv2\n# import random\nimport glob\nimport PIL\nfrom PIL import Image\nfrom tqdm import tqdm\nimport imghdr\nfrom patchify import patchify \n\n\nimport time\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport albumentations as A\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import ConvTranspose2d\nfrom torch.nn import Conv2d\nfrom torch.nn import MaxPool2d\nfrom torch.nn import Module\nfrom torch.nn import ModuleList\nfrom torch.nn import ReLU\nfrom torch.nn import Dropout\nfrom torchsummary import summary\n\nfrom torch.nn import BatchNorm2d \n\nfrom torchvision.transforms import CenterCrop\nfrom torch.nn import functional as F\nfrom torch.nn.functional import normalize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-25T13:11:15.644419Z","iopub.execute_input":"2024-02-25T13:11:15.644821Z","iopub.status.idle":"2024-02-25T13:12:14.700287Z","shell.execute_reply.started":"2024-02-25T13:11:15.644787Z","shell.execute_reply":"2024-02-25T13:12:14.698956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- source_tutorial: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n- unet: https://github.com/milesial/Pytorch-UNet\n- Probablistic_unet: https://github.com/stefanknegt/Probabilistic-Unet-Pytorch/blob/master/probabilistic_unet.py\n- utility script: https://github.com/CaptainDredge/Image-segmentation-utilities\n- GPU_utility: https://github.com/anderskm/gputil\n- torch_em: https://github.com/computational-cell-analytics/dl-for-micro/blob/main/2_cell_segmentation/torchem-train-cell-membrane-segmentation.ipynb\n- torch_em: https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unet.py\n- digital_sreeni: https://github.com/bnsreenu/python_for_microscopists/blob/master/206_sem_segm_large_images_using_unet_with_custom_patch_inference.py\n- data_Augmentatin: https://albumentations.ai/docs/getting_started/mask_augmentation/\n- learning rate https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n\n","metadata":{}},{"cell_type":"markdown","source":"### loss functionds\nThe validation loss function is just a metric and actually not needed for training. It's there because it make sense to compare the metrics which your network is actually optimzing on. So you can add any other loss function as metric during compilation and you'll see it during training.\n- https://dev.to/_aadidev/3-common-loss-functions-for-image-segmentation-545o\n- https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics/notebook\n- https://github.com/JunMa11/SegLoss \n- https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/\n- important for iunet loss and everytjing https://discuss.pytorch.org/t/unet-implementation/426/12","metadata":{}},{"cell_type":"markdown","source":"name coding: \nbSz4_1CH_ip128_vls0.2_ADM_EnDc16-512_drp_ep10_aug4\n\n- bsz: batch size\n- lR: learning rate 1e-num\n- SDG: optimiser\n- ADM: Adam optimiser\n- clcLR: scheduler \n- vls: validation split\n- EnDc: encodert decoder channels\n- pt: patience for earkly stopping\n- i/p : input size \n- ep: epochs\n- wd: weight decay(l2 reg)\n- drp: drop out added\n- 1CH: input one channel instead of RGB\n- aug: num of augmentation","metadata":{}},{"cell_type":"markdown","source":"## Setting up configuration for running in notebooks\n\n- The pin memory is set to True to the DataLoader which will automatically put the fetched data Tensors in pinned memory, enabling faster data transfer to CUDA-enabled GPU's. For every epoch the data is transferred from CPU to GPU, with augmentations done in the CPU, and trainings done in the GPU.\n- Check the output directory for saving the images, model path, plot path and test path (BASE_OUTPUT, MODEL_PATH, PLOT_PATH, TEST_PATH)\n- Note keep the bacthsize small (2,5) untill you start using pacthes of the image otherwise the GPU will run out of memory","metadata":{}},{"cell_type":"code","source":"# define the test split\n# TEST_SPLIT = 0.3\nVALIDATION_SPLIT = 0.3\n\n# determine the device to be used for training and evaluation\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# determine if we will be pinning memory during data loading\n# PIN_MEMORY = True if DEVICE == \"cuda\" else False\n\n# define the number of inputchannels in the input, number of classes,\n# and number of levels (start and end) in the U-Net model\nNUM_CHANNELS = 1\nNUM_CLASSES = 1\nNUM_LEVELS = 3\nSTART_CHANEL = 16\nEND_CHANEL = 256\nNUM_WORKERS = 2 if torch.cuda.is_available() else 1 \n\n# initialize learning rate, number of epochs to train for, and the\n# batch size ( in general small batch size has 256 samples, here due to 2 batch ss we have )\nINIT_LR = 1e-3\nNUM_EPOCHS = 10\nBATCH_SIZE = 2\nPATCH_SIZE = 128\nWEIGHT_DECAY = 1e-6\n\n# define the input image dimensions\n# INPUT_IMAGE_WIDTH = 2048\n# INPUT_IMAGE_HEIGHT = 1536\n\nINPUT_IMAGE_WIDTH = PATCH_SIZE\nINPUT_IMAGE_HEIGHT = PATCH_SIZE\nNUM_AUGMENTATION = 3\n\n#learning rate scheduler \nPatience = 4\nMAX_lr = 1e-1\nBASE_lr = 1e-6\nSTEP_SIZE = 50\n\n# define thresholds for early stopping, for accuracy calculation and predcitions \nEARLY_STOP_THRES = 3\nbest_accuracy = -1\nbest_epoch = -1\nTHRESHOLD = 0.5\n\n\nplotName = f\"bSz{BATCH_SIZE}_ip{PATCH_SIZE}_EnDc{START_CHANEL}-{END_CHANEL}_ep{NUM_EPOCHS}_vls{VALIDATION_SPLIT}_aug{NUM_AUGMENTATION}\"\n\n\nBASE_OUTPUT = \"/kaggle/working/\"\n# define the path to the output serialized model, model training\n# plot, and testing image paths\nMODEL_PATH = os.path.join(BASE_OUTPUT, f\"{plotName}\")\n# PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n# TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n\n# define the path to the base output directory\ngpu_usage() ","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:14:44.802934Z","iopub.execute_input":"2024-02-25T13:14:44.803389Z","iopub.status.idle":"2024-02-25T13:14:44.816422Z","shell.execute_reply.started":"2024-02-25T13:14:44.803329Z","shell.execute_reply":"2024-02-25T13:14:44.81519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nname = \"bSz2_ip128_EnDc16-256_ep10_vls0.3_aug3\"\n# Define the regular expression pattern\npattern = r\"(ep\\d+)\"\n# int(re.search(pattern, name).group(0))\n\n# Define the regular expression pattern to only capture the digits\npattern = r\"(ep\\d+)\"\n\n# Extract the number and convert it to an integer\nlastEpochInfo = re.search(pattern, name)[0] # Extract the entire match\nint(re.search(\"\\d+\", lastEpochInfo)[0]), lastEpochInfo\n# print(\"Extracted number:\", number)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T10:44:18.256988Z","iopub.execute_input":"2024-02-25T10:44:18.258294Z","iopub.status.idle":"2024-02-25T10:44:18.268217Z","shell.execute_reply.started":"2024-02-25T10:44:18.258247Z","shell.execute_reply":"2024-02-25T10:44:18.26689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CREATING THE CUSTOM DATASET","metadata":{}},{"cell_type":"code","source":"def readData(imgPath, labelPath, convertType):\n    \"\"\"Reads and creates a list of target\"\"\" \n    image = []\n    label = []\n    imageList = []\n    labelList = []\n  \n    for i, image_name in enumerate(sorted(os.listdir(imgPath))):\n        if ((('.').join(image_name.split('.')[-1:])== 'tif') or (('.').join(image_name.split('.')[-1:]) == 'tiff')):\n            label_name = '.'.join(image_name.split('.')[:-1]) +  '_bn.tif'\n            \n            if label_name in list(os.listdir(labelPath)): \n                # normalise by 255.0 -> convert to array -> append to list\n                img_Path = os.path.join(imgPath, image_name)\n                img = Image.open(img_Path).convert(convertType)\n                img = np.array(img, dtype = np.float32)/255.0\n                image.append(img)\n                imageList.append((img_Path))\n                \n                label_Path = os.path.join(labelPath, label_name)\n                img = Image.open(label_Path).convert(convertType)\n                label.append(np.where(np.array(img) >= 1, 1.0, 0.0))\n                labelList.append((label_Path))\n            else:\n                print('Images with no mask-->', image_name)\n        else: print('Image with new extension', image_name)\n    print(f'total images --> {len(imageList)}, total masks --> {len(labelList)}')       \n    return image, label\n            \ninput_folder = '/kaggle/input/myofarm/Images_input'\nlabel_folder = '/kaggle/input/myofarm/Images_target'\nimage, label = readData(input_folder, label_folder, convertType = 'L')","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:14.564336Z","iopub.execute_input":"2024-02-25T13:17:14.56475Z","iopub.status.idle":"2024-02-25T13:17:41.956829Z","shell.execute_reply.started":"2024-02-25T13:17:14.564721Z","shell.execute_reply":"2024-02-25T13:17:41.955643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"/kaggle/input/myofarm/Images_target/20220714_JK_d1_diff_isRASb1_corr_13_5_uM_CHIR_220018_p17_6mio_dish_2_bn.tif\n\n'/kaggle/input/myofarm/Images_target/20230306_JK_230008_d12_isWT1Bld_2_261_bn.tif'\n'/kaggle/input/myofarm/Images_target/20230217_JK_BRAF_mc_8_p23_d9_230007_bn.tif'","metadata":{}},{"cell_type":"code","source":"def plotSanityCheckImages(img1, img2):\n    \n    print(f'Images:{img1.shape} and mask:{img2.shape} shape\\n')\n    \n    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10,10))\n    ax[0].imshow(img1)\n    ax[1].imshow(img2, cmap = 'gray')\n    ax[2].imshow(img2 * img1, cmap = 'gray')\n\n    ax[0].set_title(\"Image with the original channel\")\n    ax[1].set_title(\"Original Mask\")\n    ax[2].set_title(\"combined and made gray\")\n\n    figure.tight_layout()\n    figure.show()\n\nplotSanityCheckImages(image[0], label[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:41.958733Z","iopub.execute_input":"2024-02-25T13:17:41.959068Z","iopub.status.idle":"2024-02-25T13:17:43.670424Z","shell.execute_reply.started":"2024-02-25T13:17:41.959039Z","shell.execute_reply":"2024-02-25T13:17:43.668915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataTransform(image, mask):\n    images_list, masks_list = [], []\n    \n    transform = A.Compose([ A.HorizontalFlip(p = 0.5),\n                            A.VerticalFlip(p = 0.5),\n                            A.RandomBrightnessContrast(p=0.5),\n                            A.ElasticTransform(p=0.5),\n                            A.GridDistortion(p = 0.5), ])\n\n    for i in range(NUM_AUGMENTATION):\n        augmentations = transform(image = np.array(image), mask = np.array(mask))\n        images_list.append(augmentations[\"image\"])\n        masks_list.append(augmentations[\"mask\"])\n        \n    return images_list, masks_list\n    \nprint('To check the augmentation patterns')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.68691Z","iopub.execute_input":"2024-02-25T13:17:43.687485Z","iopub.status.idle":"2024-02-25T13:17:43.698742Z","shell.execute_reply.started":"2024-02-25T13:17:43.687432Z","shell.execute_reply":"2024-02-25T13:17:43.69742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createPatches(imgList, maskList, PATCH_SIZE):\n    images = []\n    masks = []\n    for i, (image, mask) in enumerate(zip(imgList, maskList)):      \n        patch_images = patchify(image, (PATCH_SIZE,PATCH_SIZE), step = PATCH_SIZE)\n        patch_masks = patchify(mask, (PATCH_SIZE,PATCH_SIZE), step = PATCH_SIZE)\n        \n        for i in range(patch_images.shape[0]):\n            for j in range(patch_images.shape[1]):\n                single_patch_img = patch_images[i,j,:,:]\n                images.append(single_patch_img)\n                \n                single_patch_img = patch_masks[i,j,:,:]\n                masks.append(single_patch_img)\n                \n    images = torch.reshape(torch.tensor(np.array(images)), [-1,1,PATCH_SIZE,PATCH_SIZE])       \n    masks = torch.reshape(torch.tensor(np.array(masks)), [-1,1,PATCH_SIZE,PATCH_SIZE])\n        \n    return images, masks    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.700332Z","iopub.execute_input":"2024-02-25T13:17:43.700766Z","iopub.status.idle":"2024-02-25T13:17:43.711568Z","shell.execute_reply.started":"2024-02-25T13:17:43.700724Z","shell.execute_reply":"2024-02-25T13:17:43.710375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suppose NUM_AUGMENTATION is set to 15, and you have 10 original images in your dataset. So, the total number of items in the augmented dataset would be 10 * 15 = 150.\n\nWhen idx is 25:\noriginal_idx = 25 // 15 = 1\naugmentation_idx = 25 % 15 = 10\nThis means the current index corresponds to the 10th augmentation of the 2nd original image.\nWhen idx is 43:\noriginal_idx = 43 // 15 = 2\naugmentation_idx = 43 % 15 = 13\nThis means the current index corresponds to the 13th augmentation of the 3rd original image.\n\n\nhe createPATCHES function likely processes multiple images and masks at once. By passing [augmented_img] and [augmented_mask] (which are lists), you're providing the function with the necessary format for handling multiple images and masks, even though in this case, you're passing only one of each.","metadata":{}},{"cell_type":"code","source":"class SpheroidDataset(Dataset):\n\n    def __init__(self, imagePaths, maskPaths, transforms = None, num_augmentations=NUM_AUGMENTATION):\n        self.imagePaths = imagePaths\n        self.maskPaths = maskPaths\n        self.transforms = transforms\n        self.num_augmentations = num_augmentations\n\n        # Read and store image and mask data efficiently\n        self.images, self.masks = readData(self.imagePaths, self.maskPaths, \"L\")\n\n    def __len__(self):\n        if self.transforms == True:\n            length = len(self.images) * self.num_augmentations\n        else:\n            length = len(self.images)\n        print(f'Images Augmentated-->{self.transforms}; so Dataset_length-->{length}')\n        return length \n    \n\n    def __getitem__(self, idx):\n        if self.transforms == True:\n            original_idx = idx // self.num_augmentations\n            augmentation_idx = idx % self.num_augmentations\n        else:\n            original_idx = idx \n\n        # Retrieve image and mask from pre-loaded data\n        img = self.images[original_idx]\n        mask = self.masks[original_idx]\n\n        # Transform(y/n) -> patches \n        if self.transforms == True:\n            images_list, masks_list = dataTransform(img, mask)\n            augmented_img = images_list[augmentation_idx]\n            augmented_mask = masks_list[augmentation_idx]\n            image_patch, mask_patch = createPatches([augmented_img], [augmented_mask], PATCH_SIZE)\n        else:\n            image_patch, mask_patch = createPatches([img], [mask], PATCH_SIZE)\n\n        return image_patch, mask_patch \n\n\n# input_folder = '/kaggle/input/myofarm/Images_input'\n# mask_folder = '/kaggle/input/myofarm/Images_target'\n\n# dataset = SpheroidDataset(input_folder, mask_folder, transforms = True)\n\n# for images, masks in dataset:    \n#     print(images.shape) #print path and others also for checking\n#     print(torch.min(images), torch.mean(images), torch.max(images))\n#     print(masks.shape)\n#     print('\\n')\n#     break\n    \n# print(\"Total images: \", len(dataset))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.713599Z","iopub.execute_input":"2024-02-25T13:17:43.713977Z","iopub.status.idle":"2024-02-25T13:17:43.730459Z","shell.execute_reply.started":"2024-02-25T13:17:43.713948Z","shell.execute_reply":"2024-02-25T13:17:43.729076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Images with no mask--> 20230217_JK_BRAF_mc_8_p23_d9_230007.tif\nImages with no mask--> 20230306_JK_230008_d12_isWT1Bld_2_261.tif\ntotal images --> 122\ntotal masks --> 122\ntransforming\ntorch.Size([192, 1, 128, 128])\ntensor(0.2118) tensor(0.5156) tensor(0.9176)\ntorch.Size([192, 1, 128, 128])\n\n\noriginal_length-->122\naugmented_length-->488\nTotal images:  488","metadata":{}},{"cell_type":"markdown","source":"## LOSS Functions and Accuracy metric\n","metadata":{}},{"cell_type":"code","source":"def modelAccuracy(inputs, targets):\n    # Calculate pixel-wise accuracy\n    # assuming input and targets afe falttened\n    correct = (preds == targets).float()\n    accuracy = correct.sum() / targets.numel()\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.732149Z","iopub.execute_input":"2024-02-25T13:17:43.732576Z","iopub.status.idle":"2024-02-25T13:17:43.745223Z","shell.execute_reply.started":"2024-02-25T13:17:43.73254Z","shell.execute_reply":"2024-02-25T13:17:43.744311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        # inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth) \n        accuracy = modelAccuracy(inputs, targets)\n        \n        return 1 - dice, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.746493Z","iopub.execute_input":"2024-02-25T13:17:43.747431Z","iopub.status.idle":"2024-02-25T13:17:43.756793Z","shell.execute_reply.started":"2024-02-25T13:17:43.747388Z","shell.execute_reply":"2024-02-25T13:17:43.75547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PyTorch\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        # inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n        \n        accuracy = modelAccuracy(inputs, targets)\n        \n        return Dice_BCE, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.758793Z","iopub.execute_input":"2024-02-25T13:17:43.759246Z","iopub.status.idle":"2024-02-25T13:17:43.771108Z","shell.execute_reply.started":"2024-02-25T13:17:43.759211Z","shell.execute_reply":"2024-02-25T13:17:43.76973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PyTorch\nALPHA = 0.8\nGAMMA = 2\n\nclass FocalLoss(nn.Module):\n    # loss = model_lossFunc(pred.to(torch.float32), y.to(torch.float32))\n    def __init__(self, weight=None, size_average=True):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n        #comment out if your model contains a sigmoid or equivalent activation layer\n        # inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1) #model_predictions\n        targets = targets.view(-1) #originalMasks\n        \n        #first compute binary cross-entropy \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n        \n        accuracy = modelAccuracy(inputs, targets)\n                       \n        return focal_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.774651Z","iopub.execute_input":"2024-02-25T13:17:43.775315Z","iopub.status.idle":"2024-02-25T13:17:43.784585Z","shell.execute_reply.started":"2024-02-25T13:17:43.775269Z","shell.execute_reply":"2024-02-25T13:17:43.783322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building UNET\n\n- Overall, our U-Net model will consist of an Encoder class and a Decoder class. The encoder will gradually reduce the spatial dimension to compress information. Furthermore, it will increase the number of channels, that is, the number of feature maps at each stage, enabling our model to capture different details or features in our image. On the other hand, the decoder will take the final encoder representation and gradually increase the spatial dimension and reduce the number of channels to finally output a segmentation mask of the same spatial dimension as the input image.\n\n- Next, we define a Block module as the building unit of our encoder and decoder architecture. It is worth noting that all models or model sub-parts that we define are required to inherit from the PyTorch Module class, which is the parent class in PyTorch for all neural network modules.","metadata":{}},{"cell_type":"markdown","source":"## trial model with channel info outside ","metadata":{}},{"cell_type":"code","source":"def UNETchannels(startChanel, endChanel, num_chanel = NUM_CHANNELS):\n    startPower = int(math.log2(startChanel))\n    endPower = int(math.log2(endChanel))\n    enCh = [num_chanel]\n    deCh = []\n    for num in range(startPower, endPower+1):\n        enCh.append(2**num)\n    deCh = enCh[::-1]\n    deCh.pop()\n    return tuple(enCh), tuple(deCh)\n\n# START_CHANEL = 16\n# END_CHANEL = 1024\nenCh, deCh = UNETchannels(START_CHANEL, END_CHANEL)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.786052Z","iopub.execute_input":"2024-02-25T13:17:43.786423Z","iopub.status.idle":"2024-02-25T13:17:43.799319Z","shell.execute_reply.started":"2024-02-25T13:17:43.786392Z","shell.execute_reply":"2024-02-25T13:17:43.798081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(Module):\n    def __init__(self, inChannels, outChannels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(inChannels, outChannels,\n                            kernel_size=3, stride=1, padding=1, bias=True)\n        self.BN1 = nn.BatchNorm2d(outChannels)\n        self.relu1 = nn.ReLU(inplace = True)\n        self.dropout = nn.Dropout(0.25)\n            \n        self.conv2 = Conv2d(outChannels, outChannels, \n                            kernel_size=3, stride=1, padding=1, bias=True)\n        self.BN2 = BatchNorm2d(outChannels)\n        self.relu2 = ReLU(inplace = True)\n        \n    def forward(self, x):\n        # apply CONV => [BN] => RELU => CONV block to the inputs and return it\n        outputConv1 = self.dropout(self.relu1(self.BN1(self.conv1(x))))\n        outputConv2 = self.relu2(self.BN2(self.conv2(outputConv1))) \n        return outputConv2","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.801035Z","iopub.execute_input":"2024-02-25T13:17:43.801876Z","iopub.status.idle":"2024-02-25T13:17:43.812163Z","shell.execute_reply.started":"2024-02-25T13:17:43.801831Z","shell.execute_reply":"2024-02-25T13:17:43.811171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(Module):    \n    def __init__(self, channels = enCh): \n        super().__init__()\n        self.encBlocks = ModuleList([Block(channels[i], channels[i + 1]) \n                                      for i in range(len(channels) - 1)])\n        self.pool = MaxPool2d(kernel_size=2, stride=2)\n\n    # this takes an input image x\n    # initialize an empty list (blockOutputs) to store the intermediate outputs passed\n    # to the decoder where they can be processed with the decoder feature maps.   \n    def forward(self, x): \n        blockOutputs = []\n        for block in self.encBlocks:\n            x = block(x)\n            blockOutputs.append(x)\n            x = self.pool(x)\n\n        return blockOutputs\n    \n    \nclass Decoder(Module):\n    def __init__(self, channels = deCh):\n        super().__init__()\n\n        self.channels = channels\n        self.upconvs = ModuleList([ConvTranspose2d(channels[i], channels[i + 1], kernel_size=2, stride=2)\n                                   for i in range(len(channels) - 1)])\n        self.dec_blocks = ModuleList([Block(channels[i], channels[i + 1])\n                                       for i in range(len(channels) - 1)])\n\n    # Input is the feature map x and \n    # the list of intermediate outputs from the encoder \n    def forward(self, x, encFeatures):\n\n        for i in range(len(self.channels) - 1):\n            x = self.upconvs[i](x)\n            # crop the current features from the encoder blocks,\n            # concatenate them with the current upsampled features,\n            # and pass the concatenated output through the current decoder block\n            # encFeat = self.crop(encFeatures[i], x)\n            x = torch.cat([x, encFeatures[i]], dim=1)\n            x = self.dec_blocks[i](x)\n\n        return x  \n\nclass UNet(Module):\n        \n    def __init__(self, encChannels = enCh,\n                 decChannels = deCh, \n                 nbClasses=1, retainDim=True,\n                 outSize=(INPUT_IMAGE_HEIGHT,  INPUT_IMAGE_WIDTH)):\n        super().__init__()\n        self.encoder = Encoder(encChannels)\n        self.decoder = Decoder(decChannels)\n        self.head = nn.Sequential(nn.Conv2d(decChannels[-1], nbClasses, kernel_size=1),\n                                  nn.Sigmoid() )\n        self.retainDim = retainDim\n        self.outSize = outSize\n        \n        \n    def forward(self, x):\n        # grab the features from the encoder\n        # Note that the encFeatures list contains \n        # all the feature maps starting from the first encoder block output to the last\n        encFeatures = self.encoder(x)\n        # pass the encoder features through decoder making sure that\n        # their dimensions are suited for concatenation\n        # since the encoder feature maps starting from the last encoder block output to the first of the decoder\n       \n        # output of the final encoder block \n        # (i.e., encFeatures[::-1][0]) and the feature map outputs of all intermediate encoder blocks \n        # (i.e., encFeatures[::-1][1:]) to the decoder\n        decFeatures = self.decoder(encFeatures[::-1][0], encFeatures[::-1][1:])\n        map = self.head(decFeatures)\n        \n        if self.retainDim:\n            map = F.interpolate(map, self.outSize)\n\n        return map   \nsummary(UNet().to(DEVICE), input_size=(1,PATCH_SIZE,PATCH_SIZE))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:43.814149Z","iopub.execute_input":"2024-02-25T13:17:43.814634Z","iopub.status.idle":"2024-02-25T13:17:44.233301Z","shell.execute_reply.started":"2024-02-25T13:17:43.814593Z","shell.execute_reply":"2024-02-25T13:17:44.232187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- nbClasses: This defines the number of segmentation classes where we have to classify each pixel. This usually corresponds to the number of channels in our output segmentation map, where we have one channel for each class. Since we are working with two classes (i.e., binary classification), we keep a single channel and use thresholding for classification, as we will discuss later.\n\n- Now the encFeatures[::-1] list contains the feature map outputs in reverse order (i.e., from the last to the first encoder block). Note that this is important since, on the decoder side, we will be utilizing the encoder feature maps starting from the last encoder block output to the first.","metadata":{}},{"cell_type":"markdown","source":"## load previously trained model\n","metadata":{}},{"cell_type":"code","source":"def loadOldModel(path, newModel = None):\n    if newModel == True:\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os #/kaggle/input/test_myotwin/pytorch/unet-bsz2_ip128_endc16-256_ep10/1/unet_spheroid-3.pth\nos.listdir(\"/kaggle/input/test_myotwin/pytorch/v2fromver56/2\") #/kaggle/input/test_myotwin/pytorch/v2fromver56/2\nos.list(\"/kaggle/input/test_myotwin/pytorch/v2fromver56/2/bSz2_ip128_EnDc16-256_ep10_vls0.3_aug3_FullModel.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:03:46.487938Z","iopub.execute_input":"2024-02-25T14:03:46.488371Z","iopub.status.idle":"2024-02-25T14:03:46.504712Z","shell.execute_reply.started":"2024-02-25T14:03:46.48834Z","shell.execute_reply":"2024-02-25T14:03:46.503748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## checkpoints and saving of model ","metadata":{}},{"cell_type":"code","source":"# this is for the full model saving \ndef save_checkpoint(state, filename):\n    print(\"=> Saving complete model \")\n    filename = filename + '_FullModel.pth'\n    torch.save(state, filename)\n    return filename\n    \n# this is just to save the model parameters not very comprehensive\ndef checkpoint_inference(model, epoch, optimizer, filename):\n    print(\"=> Saving all model parameters \")\n    filename = filename + '_ModelInference.pth'\n    torch.save({\n            'model': unet,\n            'epoch': epoch+1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()}, filename)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:44.235151Z","iopub.execute_input":"2024-02-25T13:17:44.236302Z","iopub.status.idle":"2024-02-25T13:17:44.245007Z","shell.execute_reply.started":"2024-02-25T13:17:44.236258Z","shell.execute_reply":"2024-02-25T13:17:44.243797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the UNET model","metadata":{}},{"cell_type":"markdown","source":"- ToPILImage(): it enables us to convert our input images to PIL image format. Note that this is necessary since we used OpenCV to load images in our custom dataset, but PyTorch expects the input image samples to be in PIL format.\n- Resize(): allows us to resize our images to a particular input dimension (i.e., config.INPUT_IMAGE_HEIGHT, config.INPUT_IMAGE_WIDTH) that our model can accept\n- ToTensor(): enables us to convert input images to PyTorch tensors and convert the input PIL Image, which is originally in the range from [0, 255], to [0, 1].\n","metadata":{}},{"cell_type":"code","source":"# BATCH_SIZE = 5\n# VALIDATION_SPLIT = 0.2\ntrain_dataset = SpheroidDataset(input_folder, label_folder, transforms = True)\ntrain_set, validation_set = torch.utils.data.random_split(train_dataset, \n                                                          [round(len(train_dataset)*(1-VALIDATION_SPLIT)), \n                                                           round(len(train_dataset)*VALIDATION_SPLIT)])\n\ntrainLoader = DataLoader(dataset = train_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle =True)\nvalidationLoader = DataLoader(dataset = validation_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle =True) \n\n# calculate steps per epoch for training and test set\ntrainSteps = len(train_set) // BATCH_SIZE\ntestSteps = len(validation_set) // BATCH_SIZE\nvalidationSteps = len(validation_set) // BATCH_SIZE\n\nprint(f\"Total {len(train_set)} instances in the training and {trainSteps} per epoch\")\nprint(f\"Total {len(validation_set)} instances in the validation set and {validationSteps} per epoch\")\nprint(f\"Each Epoch runs: {len(trainLoader)} times, where each loader has {BATCH_SIZE} instances for trainig\")\n\ngpu_usage() ","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:44.246543Z","iopub.execute_input":"2024-02-25T13:17:44.247041Z","iopub.status.idle":"2024-02-25T13:17:53.665022Z","shell.execute_reply.started":"2024-02-25T13:17:44.247006Z","shell.execute_reply":"2024-02-25T13:17:53.664009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nname = \"bSz2_ip128_EnDc16-256_ep10_vls0.3_aug3\"\n# Define the regular expression pattern\npattern = r\"(ep\\d+)\"\n# int(re.search(pattern, name).group(0))\n\n# Define the regular expression pattern to only capture the digits\npattern = r\"(ep\\d+)\"\n\n# Extract the number and convert it to an integer\nlastEpochInfo = re.search(pattern, name)[0] # Extract the entire match\nint(re.search(\"\\d+\", lastEpochInfo)[0]), lastEpochInfo\n# print(\"Extracted number:\", number)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:17:53.666367Z","iopub.execute_input":"2024-02-25T13:17:53.666908Z","iopub.status.idle":"2024-02-25T13:17:53.675981Z","shell.execute_reply.started":"2024-02-25T13:17:53.666878Z","shell.execute_reply":"2024-02-25T13:17:53.674215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oldModel = str(os.listdir('/kaggle/input/test_myotwin/pytorch'))\npattern = r\"(ep\\d+)\"\ntrainedEpoch = re.search(pattern, oldModel)[0]\ntrainedEpoch = re.search(\"\\d+\", lastEpochInfo)[0]\nprint(trainedEpoch)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T10:56:23.765053Z","iopub.execute_input":"2024-02-25T10:56:23.765488Z","iopub.status.idle":"2024-02-25T10:56:23.773083Z","shell.execute_reply.started":"2024-02-25T10:56:23.765455Z","shell.execute_reply":"2024-02-25T10:56:23.771926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newModel = True\npath = '/kaggle/input/test_myotwin/pytorch/unet-bsz2_ip128_endc16-256_ep10/1/unet_spheroid-3.pth'\nif newModel == True:\n#     unet = torch.load(path).to(DEVICE)\n    print('Model Loaded')","metadata":{"execution":{"iopub.status.busy":"2024-02-25T10:45:52.812474Z","iopub.execute_input":"2024-02-25T10:45:52.812874Z","iopub.status.idle":"2024-02-25T10:45:52.818782Z","shell.execute_reply.started":"2024-02-25T10:45:52.812828Z","shell.execute_reply":"2024-02-25T10:45:52.817608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# initialize loss function and optimizerimizer\nmetric_lossFunc = DiceBCELoss() #nn.BCELoss() #DiceLoss()\noptimizer = optim.Adam(unet.parameters(), lr= INIT_LR, weight_decay = WEIGHT_DECAY)\n# optimizer = torch.optim.SGD(unet.parameters(), lr=INIT_LR, momentum=0.9, weight_decay = WEIGHT_DECAY)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor= 0.1, patience=Patience, verbose = True)\n# scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1, cycle_momentum = True, verbose = True)\n\n# initialize a dictionary to store training history\nH = {\"train_loss\": [], \"validation_loss\": [], \"model_accuracy\":[]}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-20T14:40:17.64381Z","iopub.execute_input":"2024-02-20T14:40:17.644994Z","iopub.status.idle":"2024-02-20T14:40:17.87808Z","shell.execute_reply.started":"2024-02-20T14:40:17.644962Z","shell.execute_reply":"2024-02-20T14:40:17.877161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = 0.1234567890\nprint(f\"{x:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T09:36:11.012781Z","iopub.execute_input":"2024-02-25T09:36:11.013252Z","iopub.status.idle":"2024-02-25T09:36:11.019317Z","shell.execute_reply.started":"2024-02-25T09:36:11.01322Z","shell.execute_reply":"2024-02-25T09:36:11.018001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loop over epochs\nprint(\"[INFO] training the network...\")\nstartTime = time.time()\nfor epoch in tqdm(range(NUM_EPOCHS)):\n    # set the model in training mode - initialise train and val loss - loop on train set\n    unet.train()\n    totalTrainLoss = 0\n    totalValidationLoss_model = 0\n    totalValidationLoss_metric = 0\n    totalAccuracy = 0\n    print(f\"the model runs -> {len(trainLoader)} per epoch {epoch}\")\n    \n    for (i, (x, y)) in enumerate(trainLoader):\n        # print(f\"\\ntrainLoader enumerated #_{i}_section\\n\")\n        # send the input to the device - forward pass - train loss - zero out prev gradients \n        # - back propagation - update model params - add train loss (.item makes it float)\n        #\"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n        x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n        y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n        #print(x.shape)\n        pred = unet(x) \n        loss, accuracy = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        totalTrainLoss += loss.item()\n        \n    # switch off autograd for validation set\n    with torch.no_grad():\n        unet.eval()\n        for (x, y) in validationLoader:\n            \"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n            x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n            y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n            pred = unet(x)\n            loss, acc = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32)).item()\n            totalValidationLoss_metric += loss\n            totalAccuracy += acc\n            \n    # calculate the average training and validation loss and binary accuracy\n    avgTrainLoss = totalTrainLoss / trainSteps\n    avgValidationLoss = totalValidationLoss_metric / validationSteps\n    avgAcc = totalAccuracy / validationSteps\n    \n    H[\"train_loss\"].append(avgTrainLoss)\n    H[\"validation_loss\"].append(avgValidationLoss) \n    H[\"model_accuracy\"].append(avgAcc) \n    \n    # print the model training and validation information\n    print(f\"For EPOCH: {epoch + 1}/{NUM_EPOCHS}, and model goes through data->{len(trainLoader)} times)\")\n    print(f\"Train loss: {avgTrainLoss:.6f}, Validation loss: {avgValidationLoss:.4f},Val accuracy: {avgAcc:.4f}%\")\n    gpu_usage()\n    \n    # saving the intermidiary informations\n    checkpoint_iference(model, epoch, optimizer, filename, newModel = None)\n\ndef checkpoint_inference(model, epoch, optimizer, filename, newModel = None):\n    \n    if newModel == True:\n          totalEpochs = \n          \n    print(\"=> Saving all model parameters \")\n    filename = filename + '_ModelInference.pth'\n    torch.save({\n            'model': unet,\n            'epoch': epoch+1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()}, filename)\n\n    # terminate the training loop\n#     if avgAcc > best_accuracy:\n#         best_accuracy = avgAcc\n#         best_epoch = epoch\n#         checkpoint(unet, MODEL_PATH)\n#     elif epoch - best_epoch > EARLY_STOP_THRES:\n#         print(\"Early stopped training at epoch %d\" % epoch)\n#         break  \n    \n    # Step the scheduler based on training loss \n#     scheduler.step(avgTrainLoss)\n    # Step the scheduler based on validation loss \n#     scheduler.step(avgValidationLoss)\n    \n# display the total time needed to perform the training\nendTime = time.time()\nprint(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:19:37.919501Z","iopub.execute_input":"2024-02-25T13:19:37.919937Z","iopub.status.idle":"2024-02-25T13:19:37.939956Z","shell.execute_reply.started":"2024-02-25T13:19:37.919908Z","shell.execute_reply":"2024-02-25T13:19:37.93886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the final model and check_point inference if the mdel training completes with no issues\nMODEL_PATH = save_checkpoint(unet, MODEL_PATH)\ncheckpoint_inference(unet,  epoch, optimizer, MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T13:19:00.030009Z","iopub.execute_input":"2024-02-25T13:19:00.03046Z","iopub.status.idle":"2024-02-25T13:19:00.079193Z","shell.execute_reply.started":"2024-02-25T13:19:00.030426Z","shell.execute_reply":"2024-02-25T13:19:00.077629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(H[\"train_loss\"], label=\"train_loss\")\nplt.plot(H[\"validation_loss\"], label=\"validation_loss\")\nplt.plot(H[\"model_accuracy\"], label=\"accuracy\")\nplt.title(f\"Losses for version:-\\n{plotName}\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"lower left\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-20T15:23:36.571912Z","iopub.execute_input":"2024-02-20T15:23:36.5722Z","iopub.status.idle":"2024-02-20T15:23:36.928916Z","shell.execute_reply.started":"2024-02-20T15:23:36.572175Z","shell.execute_reply":"2024-02-20T15:23:36.927992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model used for prediction of segemnted images","metadata":{}},{"cell_type":"code","source":"\ndef prepare_plot(origImage, origMask, predMask, file, threshold = None):\n    # initialize our figure\n    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n    ax[0].imshow(origImage)\n    ax[1].imshow(origMask)\n    ax[2].imshow(predMask)\n#     ax[3].imshow(predMask > (predMask.max() - threshold))\n#     ax[3].hist(predMask.flatten()*255)\n\n    ax[0].set_title(f\"{file}\")\n    ax[1].set_title(\"Original Mask\")\n    ax[2].set_title(f\"P_Mask-{threshold}thresh\")\n#     ax[3].set_title('P_mask w threshold')\n\n    figure.tight_layout()\n    figure.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:00:24.837275Z","iopub.execute_input":"2024-02-25T08:00:24.838138Z","iopub.status.idle":"2024-02-25T08:00:24.844983Z","shell.execute_reply.started":"2024-02-25T08:00:24.838109Z","shell.execute_reply":"2024-02-25T08:00:24.844097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, file, imagePath, groundTruthPath, patch_size, threshold = None):\n    model.eval()\n    with torch.no_grad():\n        image = Image.open(imagePath).convert(\"L\")\n        image = np.float32(image)/255.0\n        \n        gtMask = np.array(Image.open(groundTruthPath).convert(\"L\"))\n        gtMask[gtMask > 0] = 1.0\n    \n        height, width = image.shape[:2]\n        segm_img = np.zeros((height, width), dtype=np.uint8)  # Array with zeros to be filled with segmented values\n    \n        patch_num = 1\n    \n        for i in range(0, height, patch_size):  \n            for j in range(0, width, patch_size):  \n                single_patch = image[i:i+patch_size, j:j+patch_size]\n                single_patch = np.expand_dims(np.expand_dims(single_patch, 0), 0)\n                orig_patch = single_patch.copy()\n                single_patch = torch.from_numpy(single_patch).to(DEVICE)\n                \n                Mask_patch = gtMask[i:i+patch_size, j:j+patch_size]\n               \n                # pass the results through the sigmoid if the last layer of the model doesnot do sigmoid conversion\n                # single_patch_prediction = torch.sigmoid(model(single_patch)).squeeze().cpu().numpy()\n                single_patch_prediction = model(single_patch).squeeze().cpu().numpy()\n                \n                # filter out the weak predictions and convert them to integers\n                single_patch_prediction = ((single_patch_prediction>threshold)*255).astype(np.uint8)\n                \n                single_patch_shape = single_patch_prediction.shape[:2]\n                segm_img[i:i+single_patch_shape[0], j:j+single_patch_shape[1]] += cv2.resize(single_patch_prediction, single_patch_shape[::-1])\n                # print(\"Finished processing patch number \", patch_num, \" at position \", i, j)\n            \n            patch_num += 1  \n    # print(f\"Finised-{file}\")\n    prepare_plot(image, gtMask, segm_img, file, threshold)\n    return single_patch_prediction","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:02:10.455252Z","iopub.execute_input":"2024-02-25T08:02:10.455635Z","iopub.status.idle":"2024-02-25T08:02:10.466888Z","shell.execute_reply.started":"2024-02-25T08:02:10.455607Z","shell.execute_reply":"2024-02-25T08:02:10.466033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"[INFO] load up model...\")\n\nprint(\"[INFO] loading up test image paths...\\n\")\ntest_input = '/kaggle/input/myofarm/test_inputs'\ntest_mask = '/kaggle/input/myofarm/test_masks'\n\nfiles = os.listdir(test_input)\nfor file in files:\n    imagePath = test_input+ '/' + file\n    groundTruthPath = test_mask + '/' + '.'.join(file.split('.')[:-1]) + '_bn.tif'\n    simg = make_predictions(unet, file, imagePath, groundTruthPath, PATCH_SIZE, threshold = 0.5)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T08:02:26.90827Z","iopub.execute_input":"2024-02-25T08:02:26.909139Z","iopub.status.idle":"2024-02-25T08:02:34.036837Z","shell.execute_reply.started":"2024-02-25T08:02:26.909108Z","shell.execute_reply":"2024-02-25T08:02:34.035867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# imagePaths = []\n\n\n# test_input = '/kaggle/input/myofarm/test_inputs'\n# test_mask = '/kaggle/input/myofarm/test_masks'\n# files = np.random.choice(os.listdir(test_input), size = 4)\n# for file in files:\n#     imagePath = test_input+ '/' + file\n#     image = np.array(Image.open(imagePath).convert(\"L\"))\n#     plt.imshow(image)\n#     plt.show()\n#     groundTruthPath = test_mask + '/' + '.'.join(file.split('.')[:-1]) + '_bn.tif'\n#     image = np.array(Image.open(groundTruthPath).convert(\"L\"))\n#     image[image > 0] = 1.0\n#     plt.imshow(image)\n#     plt.show()\n#     print(imagePath, groundTruthPath)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T09:07:35.552565Z","iopub.execute_input":"2024-02-22T09:07:35.553101Z","iopub.status.idle":"2024-02-22T09:07:39.73638Z","shell.execute_reply.started":"2024-02-22T09:07:35.553062Z","shell.execute_reply":"2024-02-22T09:07:39.734947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}